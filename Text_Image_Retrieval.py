import os
from PIL import Image
import torch
from transformers import BlipProcessor, BlipModel
import torch.nn.functional as F
from tqdm import tqdm

# è®¾ç½®è®¾å¤‡ä¸æ¨¡å‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "Salesforce/blip-itm-base-coco"
model = BlipModel.from_pretrained(model_name).to(device)
processor = BlipProcessor.from_pretrained(model_name)

# Step 1: åŠ è½½å›¾åƒé›†å¹¶æå–ç‰¹å¾ï¼ˆä»…åšä¸€æ¬¡ï¼‰
image_folder = r"D:\RSRD-dense\train\2023-04-08-04-46-16\left"
image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(".jpg")]

image_features = []
image_names = []

for path in tqdm(image_paths[:100], desc="æå–å›¾åƒç‰¹å¾"):  # é™åˆ¶å‰100å¼ ï¼Œé˜²æ­¢çˆ†æ˜¾å­˜
    try:
        image = Image.open(path).convert("RGB")
        inputs = processor(images=image, return_tensors="pt").to(device)
        with torch.no_grad():
            feat = model.get_image_features(**inputs)
            feat = F.normalize(feat, dim=-1)  # å½’ä¸€åŒ–ç‰¹å¾
        image_features.append(feat)
        image_names.append(os.path.basename(path))
    except Exception as e:
        print(f"âŒ {path}: {e}")

image_features = torch.cat(image_features, dim=0)  # shape: [num_images, feature_dim]

# Step 2: è¾“å…¥ä¸€å¥æ–‡æœ¬ âœ æ£€ç´¢åŒ¹é…å›¾åƒ
query = "Potholed road"  # â† ä½ å¯ä»¥æ”¹æˆä¸­æ–‡å¦‚â€œå‘æ´¼è·¯é¢â€
text_inputs = processor(text=query, return_tensors="pt").to(device)
with torch.no_grad():
    text_feature = model.get_text_features(**text_inputs)
    text_feature = F.normalize(text_feature, dim=-1)

# Step 3: è®¡ç®—ç›¸ä¼¼åº¦
similarity = torch.matmul(image_features, text_feature.T).squeeze()  # shape: [num_images]

# Step 4: è·å–Top-Kç»“æœ
top_k = 5
top_indices = similarity.topk(top_k).indices.tolist()

print("\nğŸ” æŸ¥è¯¢ç»“æœï¼ˆTop-Kå›¾åƒï¼‰:")
for i in top_indices:
    print(f"{image_names[i]}  -  ç›¸ä¼¼åº¦: {similarity[i].item():.4f}")
